#!/bin/sh

curl "$@" \
  -X PUT \
  -d '{
    "operation":
      "read from csv file \"100k.csv\" separator \"\\t\" null \"<NULL>\" (
        poller string not null,
        capture_begin u64 not null,
        capture_end u64 not null,
        device_client u8 null,
        device_server u8 null,
        vlan_client u32 null,
        vlan_server u32 null,
        mac_client u64 null,
        mac_server u64 null,
        zone_client u32 not null,
        zone_server u32 not null,
        ip4_client u32 null,
        ip6_client i128 null,
        ip4_server u32 null,
        ip6_server i128 null,
        ip4_external u32 null,
        ip6_external i128 null,
        port_client u16 not null,
        port_server u16 not null,
        diffserv_client u8 not null,
        diffserv_server u8 not null,
        os_client u8 null,
        os_server u8 null,
        mtu_client u16 null,
        mtu_server u16 null,
        captured_pcap string null,
        application u32 not null,
        protostack string null,
        uuid string null,
        traffic_bytes_client u64 not null,
        traffic_bytes_server u64 not null,
        traffic_packets_client u64 not null,
        traffic_packets_server u64 not null,
        payload_bytes_client u64 not null,
        payload_bytes_server u64 not null,
        payload_packets_client u64 not null,
        payload_packets_server u64 not null,
        retrans_traffic_bytes_client u64 null,
        retrans_traffic_bytes_server u64 null,
        retrans_payload_bytes_client u64 null,
        retrans_payload_bytes_server u64 null,
        syn_count_client u64 null,
        fin_count_client u64 null,
        fin_count_server u64 null,
        rst_count_client u64 null,
        rst_count_server u64 null,
        timeout_count u64 not null,
        close_count u64 null,
        dupack_count_client u64 null,
        dupack_count_server u64 null,
        zero_window_count_client u64 null,
        zero_window_count_server u64 null,
        ct_count u64 null,
        ct_sum u64 not null,
        ct_square_sum u64 not null,
        rt_count_server u64 null,
        rt_sum_server u64 not null,
        rt_square_sum_server u64 not null,
        rtt_count_client u64 null,
        rtt_sum_client u64 not null,
        rtt_square_sum_client u64 not null,
        rtt_count_server u64 null,
        rtt_sum_server u64 not null,
        rtt_square_sum_server u64 not null,
        rd_count_client u64 null,
        rd_sum_client u64 not null,
        rd_square_sum_client u64 not null,
        rd_count_server u64 null,
        rd_sum_server u64 not null,
        rd_square_sum_server u64 not null,
        dtt_count_client u64 null,
        dtt_sum_client u64 not null,
        dtt_square_sum_client u64 not null,
        dtt_count_server u64 null,
        dtt_sum_server u64 not null,
        dtt_square_sum_server u64 not null,
        dcerpc_uuid string null
      )"
  }' \
  -H 'Content-Type: application/json' http://localhost:29380/node/TCPv29 &&

# Example that shows that:
# - in the commit-when clause, the default tuple is out not in (max
#   capture_begin refers to the vetted out tuple);
# - the commit-when clause can have its own aggregate build on others,
#   in, out...
# - the group by expression would like to refer to out-tuple aliases but
#   it's not possible ATM. To make it possible would require:
#   - instead of storing the first in-tuple with the aggregate, store
#     the first out-tuple-of-in-tuple (with aggregate fields set to their
#     init value)
#   - recompute this out-tuple for each incoming in-tuple
#   - pass it to the key function
#   - most of the time, drop it, since we'd find a pending aggregate
#   - when committing the value, pass both the first in-tuple and the
#     out-tuple (that we just computed) to the outputer so we can save
#     the computation of the alias.
#   The only time when this would save computation is when we create a new
#   aggregate. In other words, for each aggregate we are going to compute
#   the minute field twice (once to compute the key of the first aggregate
#   and another time when emitting the final value).
# TODO: also test:
#       - "commit when out.packets > 100"
#       - "commit when last.capture_begin > max(capture_begin) + 60000000"
curl "$@" \
  -X PUT \
  -d '{
    "operation":
      "SELECT MIN capture_begin, MAX capture_end, ip4_server,
              SUM(traffic_bytes_client + traffic_bytes_server) AS bytes,
              SUM(traffic_packets_client + traffic_packets_server) AS packets
       WHERE ip4_client IS NOT NULL AND ip4_server IS NOT NULL
       GROUP BY ip4_server, capture_begin//60_000_000
       COMMIT AND FLUSH WHEN MAX(capture_begin) > min_capture_begin + 10_000_000"
  }' \
  -H 'Content-Type: application/json' http://localhost:29380/node/PerServerTraffic &&

curl "$@" -X PUT http://localhost:29380/link/TCPv29/PerServerTraffic &&

curl "$@" \
  -X PUT \
  -d '{
    "operation":
      "SELECT * WHERE bytes / (min_capture_begin - max_capture_end) < 1_000_000_000"
  }' \
  -H 'Content-Type: application/json' http://localhost:29380/node/LowTrafficServers &&

curl "$@" -X PUT http://localhost:29380/link/PerServerTraffic/LowTrafficServers &&

curl "$@" \
  -X PUT \
  -d '{
    "operation":
      "ALERT \"fire brigade\"
             SUBJECT \"Low traffic to ${ip4_server}\"
             TEXT \"Since ${min_capture_begin} ${ip4_server} exchanged
                    only ${bytes} bytes (in ${packets} packets) with the
                    rest of the world.\""
  }' \
  -H 'Content-Type: application/json' http://localhost:29380/node/LowTrafficAlert &&

curl "$@" -X PUT http://localhost:29380/link/LowTrafficServers/LowTrafficAlert &&

curl "$@" http://localhost:29380/compile

curl "$@" http://localhost:29380/run
