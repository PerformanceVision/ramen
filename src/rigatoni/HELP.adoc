= Ramen / Rigatoni

== Overview

Ramen is a stream processor. Unlike other stream processors that are designed
to scale to many servers, Ramen is designed to be efficient on a single (or
very few) servers.

This document the second prototype of Ramen, named _Rigatoni_.

Rigatoni consists of a single process serving HTTP on port 29380 (by default).
Using this server one can:

- view the current configuration (ie. the stream processing graph);
- edit this configuration;
- start/stop the processing.

When it comes to processing the stream, Rigatoni does not directly execute the
graph. Rather, it compiles each operation into a separate process and those
processes then exchange the message composing the stream via shared memory ring
buffers. Rigatoni itself jut monitors those processes and manage the
configuration. This is the key idea guiding the design of Rigatoni. This is so
for performance reason: we wanted the messages to be arbitrary yet to spend
minimum time (un)serializing them and accessing their field values.

Some redundancy can be obtained from running the exact same configuration with
same input messages on two different locations (and obviously de-duplication of
the produced effects). It is outside the scope of this prototype

Protection against failure is similarly simple: any message queued will be
processed, but messages that fail will not be retried. This offer simple
protection against message-of-death scenario. We value low latency over
exhaustiveness, thus overloading the system will result in dropped messages.

The configuration consists of the stream processing graph to be implemented.
The vertices (or nodes) of this graph are the operations to be performed on the
stream and the edges (or links) are the flow of messages from one operation to
the next.  The graph is thus directed, and in general tends to be very close to
a tree but nothing prevents arbitrary graphs, including with cycles.

Messages (or events) are atomic data fragments consumed and produced by nodes.
They are processed in FIFO order (remember the ring buffers) for each
operation, but no synchronization mechanism exists between several nodes.

Most nodes receives messages as input. In that case, all messages have the same
type.  Then it process this message, and produce zero, one or more messages of
possibly another type.  Each of this produced messages will be send to all
outputs. Therefore, each node has an input and an output type: the types of
message they accept and produce.

Also since all produced messages are copied to all output, having several
output is not a way to load-balance the workload. Load balancing is handled by
Rigatoni and the configuration need not take this into account. Instead,
sending several copies of the same produced messages to different downstream
nodes is useful for conducting different computations on the same data.

In order to find a compromise between ease of use and power of expression,
messages are not totally arbitrary. They must be _tuples_, each of which fields
must be of one of the supported primitive type: character string, boolean,
floating number, and integers of various size from 8 to 128 bits. Restricting
messages to tuples simplify a lot the configuration language while not limited
the use cases too much. Actually, I'm yet to met a stream processor that can
process really arbitrary messages.

There are actually quite few types of operation one may want to perform. Or, to
put it another way: it is possible to describe many different operations with
very few construct, if they are generic enough. Rigatoni uses only 5 primitive
operations, inspired loosely by SQL.:

- +SELECT+, which filter the incoming messages and map each of the selected
  ones into an output message (not unlike the +map_filter+ operation in
  functional languages);

- +GROUP BY+, or +AGGREGATE+, which looks superficially like a +SELECT+ but
  groups incoming tuples according to some key and aggregates each group into a
  single tuple that is emitted once a condition a met. This is similar to SQL
  "SELECT ... GROUP BY" with the important difference that you have to instruct
  Rigatoni when to output the aggregation (since streams are endless, at least
  in theory, while SQL SELECTs are bounded)

- +READ+, which read messages from an external source (such as a CSV file) or
  the HTTP server.

- +YIELD+, which outputs messages without consuming any input; and therefore
  can only produce tuples of limited interest, mostly constants. This is mostly
  used for testing. One could devise a +SELECT+ operation that do not _use_
  anything from the input, but the select would still require some incoming
  message to produce anything. That's the difference with +YIELD+: As yield do
  not read any input it can produce messages at full speed.

- +OUTPUT+, which produces some externally visible result. Externally available
  tuples can be sent somewhere but can also merely be kept for an external
  program to fetch them.

== The Configuration Language Reference

We describe first values, then expressions, then operations.  All these
concepts references each others so there is no reading order that would save
you from jumping around. First reading may not be clear but everything should
eventually fall into place.

Some reserved keywords cannot be used as identifiers, unless surrounded by
simple quotes.

=== Values

==== NULLs

Like in SQL, some field values may be NULL. Rigatoni typing system knows what
value can be NULL and spare the NULL checks unless necessary.

Users can check if a NULL-able value is NULL using the +IS NULL+ or +IS NOT
NULL+ operators, which turn a NULL-able value into a boolean. This is useful
in where-clauses.

+NULL+ is both a type and a value. The +NULL+ value is the only possible value
of the +NULL+ type, or a possible value for any NULL-able type.

To write a literal +NULL+ value (of the +NULL+ type), enter `NULL`.

==== Booleans

The type for booleans is called `bool`.
Booleans true and false are spelled `true` and `false`.

==== Strings

The type for character strings is called `string`.  A literal string is double
quoted (with +"+). To include a double-quote, backslash it.  Other characters
can be backslashed: single quote (`"\'"`), newlines (`"\n"` and `"\r"`),
horizontal tab (`"\t"`) and backspace (`"\b"`), and backslash itself (`"\\"`).

==== Floats

The type for real numbers is called `float`. It is the standard IEEE.754 64
bits floats.  Literal values will cause minimum surprise: dot notation
(`"3.14"`) and scientific notation (`"314e-2"`) are supported.

==== Integers

Rigatoni allows integer types of 5 different sizes from 8 to 128 bits, signed
or unsigned: `i8`, `i16`, `i32`, `i64`, `i128`, that are signed, and `u8`,
`u16`, `u32`, `u64` and `u128`, that are unsigned.

When writing a literal integer you can specify its type by suffixing it with
the type name (for instance: `42u128` would be an unsigned integer 128 bits
wide). If you do not then Rigatoni will choose the narrowest possible type that
can accommodate that integer value.

In addition to the suffix, you can also use a cast, using the type name as a
function: `u128(42)`. This is equivalent but more general as it can be used on
other values than literal integers.

The difference between signed and unsigned is of little practical importance.
By contrast, the difference between various sizes can be of tremendous
importance. Indeed, in some occasions Rigatoni will pick a type that is
narrower than what you intended. Let's see an example:

[source,sql]
----
  SELECT status, SUM err_count AS per_status_err_count GROUP BY status
    COMMIT AND FLUSH WHEN per_status_err_count > 1000
----

Here the intend is to accumulate the error count until there are at least
1000 or them. But if err_ount is a u8, the SUM will accumulate the error
counts in an u8 which will wrap around after 255. Therefore the ending
condition (`per_status_err_count > 1000`) will never be met.  To solve this
issue, simply add a cast:

[source,sql]
----
  SELECT status, SUM u16(err_count) AS per_status_err_count GROUP BY status
    COMMIT AND FLUSH WHEN per_status_err_count > 1000
----

=== Expressions

==== Literal values

Any literal value (as described in the previous section) is a valid expression.

==== Tuple field names

In addition to literal values, one can refer to a tuple field. Which tuples are
available depends on the clause but the general syntax is:
`tuple_name.field_name`. The prefix can be omitted and then the field is
understood to refer to the input tuple (the incoming message).

Here is a list of all possible tuples:

[[input-tuple]]
===== Input tuple

The tuple that has been received as input.  Its name is `in` and that's also
the default tuple when the tuple name is omitted.

You can use the `in` tuple in all clauses but in a +YIELD+ operation or a
+READ+ operation.  When used in the commit-clause of a +GROUP BY+ operation,
it refers to the last received tuple.

[[output-tuple]]
===== Output tuple

The tuple that is going to be output. Its name is `out`.  The only place where
it can be used is in the commit-clause of a +GROUP BY+ operation. It thus
refers to the tuple that would be output shall the commit condition yields
`true`.

It is important to understand that the input and output tuples have different
types (at least in general).

[[first-tuple]]
===== First tuple

Named `first`, refers to the first tuple of an aggregation.  Can therefore only
be used in a +GROUP BY+ operation, anywhere.

Same type as the input tuple.

[[last-tuple]]
===== Last tuple

Named `last`.  Same as `first`, but refers to the last tuple aggregated in the
current bucket.

Same type as the input tuple.

Differs from `out` by its type (`out` is the current product of the operation
while `last` is the last received input tuple) and in that it can also be used
in the select-clause and the where-clause.

[[previous-tuple]]
===== Previous tuple

Named `previous`, refers to the previous version of the output tuple (the one
before `out`).

Can only be used in the commit-clause of a +GROUP BY+ operation.

Same type as the `out` tuple, obviously.

Usage example:

[source,sql]
----
  SELECT key, AND(signal) AS signal GROUP BY key COMMIT WHEN previous.signal != out.signal
----

To transform a succession of `key, signal` (where signal is a boolean) with
possibly many times the same signal value into a stream of `key, signal`
omitting the repetitions.

===== Virtual fields

In addition to the normal fields of the tuples, some tuples have 'virtual'
fields, that is fields which value is computed rather than stored. to
distinguish them from normal fields their name starts with a dash ('#'). Here
is a list of all available virtual fields and which tuple they apply to:

.Virtual Fields
|===
|Field name| Content

| `in.#count`
| How many tuples have been received (probably useless in itself but handy for comparison or with modulus).

| `out.#count`
| In a +GROUP BY+ operations, how many tuples were added so far to form the output tuple.

| `out.#successive`
| In a +GROUP BY+ operations, how many successive incoming tuples were assigned to that group (same `group by` key).
|===

==== Operators and Functions

Predefined functions can be applied to expressions to form more complex
expressions.

Here we list the available functions. There is no way to define your own
functions (short of adding them them in rigatoni source code). Therefore, there
is no real difference between 'operators' and 'functions'.

A <<table-of-precedence>> is given at the end of this section.  You can use
parentheses to group expressions.

===== Boolean operators

`and`, `or`: infix, +bool ⨉ bool -> bool+

`not`: prefix, +bool -> bool+

===== Arithmetic operators

`+`, `-`, `*`, `//`, `^`: infix, +num ⨉ num -> num+, where +num+ can be
any numeric type (integer of float).

The size of the result is the largest of the size of the operands.  Both
operands will also be converted to the largest of their type before proceeding
to the operation. For instance, in `1 + 999`, `1` will be converted to +i16+
(the type of `999`) and then a 16 bits addition will yield a 16 bits result
(regardless of wrap around). If you expect a wrap around then you need to
explicitly cast to a larger type.

Notice that `//` is the integer division

`/`: infix, floating point division, +float ⨉ float -> float+.

`%`: infix, the integer remainder, +int ⨉ int -> int+.

`abs`: prefix, absolute value, +num ⨉ num -> num+.

===== Comparison operators

`>`, `>=`, `<=`, `<`: infix, +num ⨉ num -> num+.

`=`, `!=`, `<>`: infix, +any ⨉ any -> any+, where +any+ refers to any type.

Notice that `<>` and `!=` are synonymous.

As for arithmetic operators, operand types will be enlarged to the largest
common type and the operation will return that same type.

===== Age

`age of ...` or `age(...)`. Expects its argument to be a timestamp in the UNIX
epoch and will return the difference between that timestamp and now.

===== Now

`now` returns the current timestamp as a float.

===== Sequence

`sequence` or `sequence(start)` or `sequence(start, step)`

Will output a sequence increasing (of the given `step`, or `1` by default) at
every read incoming tuple (or at every produced tuples, for +YIELD+
operations).

===== Casts

Any type name used as a function would convert its argument into that type. For
instance: `int16(42)` or `int16 of 42`.

===== Is (not) null

Turns a null-able value into a boolean. Invalid on non-null-able values.

For instance: `IS NULL NULL` is trivially true, while `IS NOT NULL some_field`
can be either true or not depending on the tuple at hand.

`IS NULL 42` is an error, though.

===== String functions

`length` returns an uint16
Use `+` for concatenation.

[[table-of-precedence]]
===== Operator precedence

From higher precedence to lower precedence:

.Table Operator precedence
|===
|Operator |Associativity

| functions
| left to right

| `not`, `is null`, `is not null`
| left to right

| `^`
| right tot left

| `*`, `//`, `/`, `%`
| left to right

| `+`, `-`
| left to right

| `>`, `>=`, `<`, `<=`, `=`, `<>`, `!=`
| left to right

| `or`, `and`
| left to right
|===

==== Aggregate functions

Aggregate functions are special functions that combines current value with
previous values instead of combining several current values.

For instance, `max response_time` will compute the max of all the response_time
fields of all incoming tuples (until the commit-clause instruct Rigatoni to
output this aggregated tuple).

===== Min, Max, Sum

Compute the `max`, `min` and `sum` of the (numeric) input values.  For `sum`,
beware that you may want a larger integer type than the one from the operand!

===== And, Or

Compute the logical `and` and `or` of the (boolean) input values.

===== First, Last

Remember only the `first` or the `last` value encountered in this aggregation.

===== Percentile

Most aggregate functions needs only to keep the current aggregate value and can
combine it with new incoming values to produce the next current aggregate.

This function is more expensive, as it requires to actually keep all
encountered values until the aggregate is flushed.

Example: `95th percentile of (response_time + data_transfert_time)`

=== Operations

==== Read

Currently the only supported syntax is:

[source,sql]
----
  READ FROM CSV FILE "path/file.csv" SEPARATOR "\t" NULL "<NULL>" (
    first_field_name field_type nullable,
    second_field_name field_type nullable,
    ...
  )
----

Which will inject the content of the given CSV file, in which field are
separated by the given string (default to coma ",") and each occurrence of the
NULL string would be assuming to be NULL (default to empty string).

Field names must be valid identifiers (aka string made of letters, underscores
and digits but as the first character), field types must be one of `bool`,
`string`, `float`, `u8`, `i8`, `u16`, etc...  and nullable must be either
`null` or `not null` to specify whether this field can be NULL or not (default
to `null`).

Example:

[source,sql]
----
READ FROM CSV FILE "/tmp/test.csv" SEPARATOR "\t" NULL "<NULL>" (
  first_name string NOT NULL,
  last_name string NOT NULL,
  year_of_birth u16 NOT NULL,
  year_of_death u16)
----

==== Yield

Syntax:

[source,sql]
----
  YIELD expression1 AS name1, expression2 AS name2, expression3 AS name3...
----

Yield merely produces an infinite stream of tuples, as fast as the downstream
nodes can consume them.

==== Select

Syntax:

[source,sql]
----
  SELECT expression1 AS name1, expression2 AS name2, ...
    WHERE where_clause
----

As a selected expression one can also use `*` (star) to mean: all _other_
fields from the input tuple (by other we mean any fields that have not be
mentioned at all in the other expression of the select-clause).

The where-clause is an expression, typically build from the input tuple, that
must have a non-nullable boolean result.

Semantic: for each input tuple, if the where-clause is true, output the tuple
build from the select-clause. If the where-clause is false then ignore the
input tuple.

==== Group by

Syntax:

[source,sql]
----
  SELECT expression1 AS name1, expression2 AS name2, ...
    GROUP BY expression3, expression4, ...
    COMMIT WHEN commit_clause
    FLUSH WHEN flush_clause
----

The select-clause is the same as above but it can use aggregate functions and,
in addition to the `in` tuple, also the `first` and `last` tuples.  Contrary to
SQL it is not an error to take a value from the input tuple in the
select-clause with no aggregation function specified. The output tuple will
merely use the current input tuple to get the value (similarly to what the
`last` aggregation function would do).

This is also what happens if you use the `*` (star) designation in the
select-clause.

The group-by-clause is a mere list of expressions that can refer to the
<<input-tuple>>, <<first-tuple>> and <<last-tuple>>. The resulting tuple will
be used as the key for the aggregation.

The commit-clause tells when the aggregation must be stopped and the aggregated
tuple outputted.

The flush-clause tells when an aggregate must be removed from memory. It is
normally when you commit the tuple, so there is a shorter syntax for it: `...
commit and flush when ...` But the extra control allows to achieve more
interesting operations.

Semantic: For each input tuple, compute the key and retrieve the current
aggregate, if any. IF not, start a new one from the initial values of each
aggregate function. Then evaluate the where-clause: if it is false, skip that
input (and discard the new aggregate that might have been created).  If the
where-clause yields true, accumulates that input into that aggregate and
compute the current output-tuple. With all this, evaluates the commit clause:
if it is true, output the output tuple. Also, and regardless of the commit
clause, evaluates the flush-clause. If it is true, deletes this aggregates and
forget about it.

Note: the actual implementation cut some corners for efficiency.

==== Output

Currently the only output command is:

[source,sql]
----
  ALERT "destination"
    SUBJECT "subject"
    TEXT "body of the message"
----

In each of the strings, `${field_name}` would be replaced by the actual value
from the input tuple.

== The API

=== Create/Modify/Delete nodes

Nodes have unique name, and their URL is `/node/the_name`.  A Node can be
created or updated by HTTP-PUTting t that URL a JSON message:

[source,json]
----
  { "operation": "... operation expression ..." }
----

For instance:

[source,json]
----
  curl -X PUT -H 'Content-Type: application/json' -d '
    { "operation":
         "SELECT first_name, last_name
            WHERE year_of_borth < 1970 AND year_of_death IS NOT NULL"
    }' http://localhost:29380/node/some_unique_name
----

The same information can be obtained back from GETting that URL, and can be
deleted with a DELETE command on that URL.

=== Create/Modify/Delete links

The URL of a link between node A and B is: `/link/A/B`.  Nodes A and B must
exist already.

The link can be created, obtained or deleted with a +PUT+, a +GET+ or a
+DELETE+ command to that URL. Notice that those messages need no body.

It is also possible to update all the links of a specific node with a +HTTP+
+PUT+ command to `/links/some_node_name` with that JSON:

----
  { "parents": [ "some_node_name", "some_other_node", ... ],
    "children": [ "some_node_name", "some_other_node", ... ] }
----

And this node configuration will be set accordingly.

=== Export/Import the whole graph

You can get the whole configuration by GETting this URL: `/graph`.  Import is
not implemented yet.

=== Compile/Start/Stop

Once your configuration is ready you can compile it by GETting `/compile`.
This will check all the operations and types in details, and generate the
executable implementing each node, or return any encountered error.

If all went well, you can then GET `/start` for Rigatoni to start all those
executables, and then `/stop` to kill them all.

